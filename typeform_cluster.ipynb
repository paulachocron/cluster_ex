{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v0gCm6EWhHXz"
   },
   "source": [
    "We need to build a model that clusters questions in a given dataset and is able to classify new ones.\n",
    "A priori, there are two directions we could take:\n",
    "\n",
    "* Use some kind of topic modeling method method to decide what texts are talking about, then decide which topics fit for new questions.\n",
    "* Use a totally unsupervised clustering method to define clusters, then use them to classify new questions.\n",
    "\n",
    "Which method to use will depend on how the questions in the dataset look, since topic modeling only makes sense for moderately long texts. Let's load the training questions and take a look at them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "tI05BbMoQsjF",
    "outputId": "bf861c8e-d607-4c70-f719-777da01cb6f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\"question\"\\n', 'In what ways did OneEleven not help you? How can we improve?                                                                                                                                                                                                                                                                                                                                                                                                                                                        \\n', 'Which is your primary time zone?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \\n', 'Can you share their contact or location with us?                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \\n', 'Customer Salary is 85000, And he is working for Indian Army and his salary account is in Andra Bank,What is the ROI, PF?                                                                                                                                                                                                                                                                                                                                                                                            \\n', 'Your foodbanks website URL:                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \\n', '\"\"\"Short Answer: Why do we need to \"\"\"\"position\"\"\"\" IPS anyway?                                                                                                                                                                                                                                                                                                                                                                                                                                                           \"\\n', 'Do you reside outside the U.S. or are you relocating out of the U.S.?                                                                                                                                                                                                                                                                                                                                                                                                                                               \\n', 'Please pop in the name of the artiste                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \\n', 'Jika ada solusi alternatif yang membantu Anda berhenti merokok dalam bentu\\xa0<strong>permen karet</strong>, apakah Anda mau menggunakannya?                                                                                                                                                                                                                                                                                                                                                                          \\n', 'Digital is a key objective in the broader people strategy                                                                                                                                                                                                                                                                                                                                                                                                                                                           \\n', 'In which year did you obtain this qualification {{answer_26899453}}?                                                                                                                                                                                                                                                                                                                                                                                                                                                \\n', 'Please select the software that was easiest to learn how to use.                                                                                                                                                                                                                                                                                                                                                                                                                                                    \\n', 'Data de fechamento do carrinho:                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \\n', 'Do you have 1 topic you really want to delve into that was not mentioned above? If so, what is the central question in that topic?                                                                                                                                                                                                                                                                                                                                                                                  \\n', 'How many, what kind?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \\n', 'Add in legal jargon.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \\n', 'Your name and lastname:                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \\n', 'Raufasertapete wird neu angebracht- zusätzlich wird weiß gestrichen. Wie viele Lagen müssen gestrichen werden?                                                                                                                                                                                                                                                                                                                                                                                                   \\n', \"{{hidden_c2}}'s Past School Information                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \\n\"]\n"
     ]
    }
   ],
   "source": [
    "questions_file = 'nlp_test.csv'\n",
    "\n",
    "with open(questions_file) as file:\n",
    "    questions = file.readlines()\n",
    "\n",
    "print(questions[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eRZGiFQmhRr8"
   },
   "source": [
    "We can make some relevant observations:\n",
    "\n",
    "* There is a column title first row that should be removed\n",
    "* There are questions in different languages\n",
    "\n",
    "Let's remove the column title and the trailing whitespaces and look at the length  of questions in more detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "qek-gyZ5hygZ",
    "outputId": "b2b5c648-d4c7-424a-9c56-de9a6e9ddf10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min len: 0\n",
      "Max len: 527\n",
      "Avg len: 96.9002\n"
     ]
    }
   ],
   "source": [
    "questions = [q.strip() for q in questions[1:]]\n",
    "qs_len = [len(q) for q in questions]\n",
    "\n",
    "print(\"Min len: {}\".format(min(qs_len)))\n",
    "print(\"Max len: {}\".format(max(qs_len)))\n",
    "print(\"Avg len: {}\".format(sum(qs_len)/float(len(qs_len))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is an empty question! Let's remove it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [q for q in questions if len(q)>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "947mAgCBiiOG"
   },
   "source": [
    "Summarizing, we have 100k short questions. The lengh (around 500 characters) rules out the option of using some kind of topic modeling: texts are too short for this. Therefore, we will stick to a clustering method. To do this, we need to find a way of encoding the texts into numbers that can be fed to a ML algorithm. In other words, we need to decide which embedding method we are going to use.\n",
    "\n",
    "One first question is how we want to take into account multilinguality. If we use different encodings for different lnguages, the clusters will almost certainly be divided by language. I will assume this is not desirable, so I will use a multilingual encoding that uses the same representation for all languages.\n",
    "\n",
    "One option is to use a language model such as BERT. These models are actually trained to perform different tasks, but embeddings can be obtained from their internal representation. For example, the library 'BERT-as-service' (https://github.com/hanxiao/bert-as-service) provides vector representations from BERT embeddings.\n",
    "\n",
    "However, there are two issues that we should take into account with this approach:\n",
    "\n",
    "* Normally, word or token based embeddings are not optimal for obtaining sentence embeddings like the ones we have. A solution is to use the token embeddings as a base to fine-tune a model for sentences (like in https://github.com/UKPLab/sentence-transformers). However, while 100k questions are enough to build a clustering model (or, for example, a classification one), it is not enough to train an embedding. There are currently no multilingual fine-tuned models available.\n",
    "* Using very large models such as BERT is slow and expensive. For example, BERT-as-service directly does not run in my CPU. We could use a framework such as Colab for training, but ultimately we need the model to encode new sentences for the HTTP service.\n",
    "\n",
    "For these reasons, I decided to use an encoder that is out-of-the-box multilingual and trained on short sentences like the ones we have: Google's Multilingual Universal Sentence Encoder, released in July 2019 (https://ai.googleblog.com/2019/07/multilingual-universal-sentence-encoder.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "171pZj3-7DeN"
   },
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "import tensorflow_text\n",
    "\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual/3\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to decide which clustering method we are going to use. There are two salient options for classifying text: HDBSCAN and k-Means. The advantage of the first method is that it does not need to know the number of clusters to be used a priori. However, I will use k-Means since it is generally preferred in practical scenarios.\n",
    "\n",
    "We also need to decide which framework/library we are going to use. We are using tensorflow to obtain the embeddings, so it makes sense to implement k-Means on it to use them directly. However, after some trying and research I discovered that the methods to load the multilingual embeddings in estimators are not yet fully implemented (see for example the following issue: https://github.com/tensorflow/hub/issues/420). For simplicity, I decided to obtain the embeddings first and then train the models separately, using Scikit-learn.\n",
    "\n",
    "Now we compute the embedding for each sentence. The Universal Sentence Encoder already performs the preprocessing, and prefers sentences as raw as possible. We do it in batches to avoid memory problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 213
    },
    "colab_type": "code",
    "id": "9rxehWTn7FAU",
    "outputId": "ade7d101-5aed-405b-f8a1-f2d913fcf073"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n"
     ]
    }
   ],
   "source": [
    "# Compute embeddings.\n",
    "qs_embeds = []\n",
    "for i in range(int(len(questions)/1000)+1):\n",
    "  if i%10==0:\n",
    "    print(i)\n",
    "  topq = min(len(questions), (i+1)*1000)\n",
    "  qs_embeds.extend(embed(questions[i*1000:topq]).numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cBzBPVaVx6VF"
   },
   "source": [
    "Next we are going to train the k-Means model. In many cases there is a previous step of dimensionality reduction that may help finding better clusters (and makes the data more manageable). Word vectors obtained from embeddings are already quite reduced, so I am going to skip this step here, but it may make sense to see whether it yields better results. Methods that could be used for this reduction are PCA or t-SNE.\n",
    "\n",
    "As we said, we do not know which is a reasonable number of clusters a priori. This will most likely depend on what the results will be used for. For now, we will make a quick analysis using the Silhouette Score, which does not require a labeled dataset, to see which number of clusters is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Md3aoRbrGOSy",
    "outputId": "f0852d51-9444-4f1f-c110-6e0a247f6778"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "\n",
    "def train_evaluate_k_means(klusters):\n",
    "    km = KMeans(n_clusters=klusters, init='k-means++', max_iter=100, n_init=1,\n",
    "                    verbose=False)\n",
    "    print(\"%s clusters\" % klusters)\n",
    "    km.fit(qs_embeds)   \n",
    "    print(\"Silhouette Coefficient: %0.3f\"\n",
    "      % metrics.silhouette_score(qs_embeds, km.labels_, sample_size=10000))\n",
    "    print()\n",
    "    return km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "42cztRQjnhPy"
   },
   "outputs": [],
   "source": [
    "for k in [25,50,100,150,200,300]:\n",
    "    train_evaluate_k_means(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results are not amazing so far, at least according to the Silhouette Score, that ranges from -1 to 1. However, we need to remember that text vectors are very special type of input, and it is very likely that this particular measure is not really meaningful for this case. Measuring quality of clusters of complex data is a difficult task, specially in a totally unsupervised method. A more suitable evaluation would arise from using the clusters for something.\n",
    "\n",
    "We will stay with the number of clusters that had better results when we tried: 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km = train_evaluate_k_means(25)\n",
    "y_kmeans = km.predict(qs_embeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have questions clustered, but we still cannot classify new data, at least without re-computing k-Means, something that we do not want to do at inference time. To achieve that, we will train a classifier using the clusters found by k-Means as labels. First let us save the questions along with their clusters, which will be useful when serving the model to find other questions in the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w1zSG_HpzcfF"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "labeled_questions = pd.DataFrame()\n",
    "labeled_questions['question'] = questions\n",
    "labeled_questions['label'] = y_kmeans\n",
    "\n",
    "labeled_questions.to_csv('clusters.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will test two different classifiers to see which one gives better results: a Random Forest and a Multi-layer Perceptron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(qs_embeds, y_kmeans, test_size=0.2, random_state=42)\n",
    "\n",
    "mlp = MLPClassifier(solver='lbfgs', alpha=1e-5, random_state=1)\n",
    "mlp.fit(X_train, y_train)\n",
    "print(\"Score of the Multi-layer perceptron classifier on the test set: {}\".format(mlp.score(X_test, y_test)))\n",
    "\n",
    "rf = RandomForestClassifier(random_state=0)\n",
    "rf.fit(X_train, y_train)\n",
    "print(\"Score of the Random Forest classifier on the test set: {}\".format(rf.score(X_test, y_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MLP wins! The good results also show the clusters are not bad. Let's train a model with the full data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.fit(qs_embeds, y_kmeans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can try to classify some new questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_questions = [\"cuantos años tienes?\", \"where do you live?\", \"what country are you in?\", \"rate our services from 0 to 10\", \"califica nuestros servicios del 0 al 10\", \"are you satisfied with our services?\", \"did you like our product?\"]\n",
    "mlp.predict(embed(new_questions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting results. The classifier selected the same cluster for the following pairs:\n",
    "(\"where do you live?\", \"what country are you in?\"), ( \"rate our services from 0 to 10\", \"califica nuestros servicios del 0 al 10\") and (\"are you satisfied with our services?\", \"did you like our product?\"). \n",
    "\n",
    "We are satisfied with these results, so we will save the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "dump(mlp, 'model.joblib') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's try to visualize the clusters we obtained. To plot our points we first need to reduce their dimentionality to 2, which we will do with t-SNE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "id": "CBhX5dmYo9rD",
    "outputId": "3a3611cb-a418-4f35-ff26-48d478f4afc9"
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tsne_init = 'pca'\n",
    "tsne_perplexity = 20.0\n",
    "tsne_early_exaggeration = 4.0\n",
    "tsne_learning_rate = 1000\n",
    "random_state = 1\n",
    "model = TSNE(n_components=2, random_state=random_state, init=tsne_init, perplexity=tsne_perplexity,\n",
    "         early_exaggeration=tsne_early_exaggeration, learning_rate=tsne_learning_rate)\n",
    "\n",
    "transformed_points = model.fit_transform(qs_embeds[:200])\n",
    "\n",
    "plt.scatter(transformed_points[:, 0], transformed_points[:, 1], c=y_kmeans[:200], s=50, cmap='viridis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "webkbGADTlfS"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "typeform_cluster.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "typenv",
   "language": "python",
   "name": "typenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
